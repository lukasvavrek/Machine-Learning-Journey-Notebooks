{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House price prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to take a look at the Boston Housing dataset, that is built-in right into the Keras. Input consists of several different information about the neighborhood with a goal of predicting the property price. Therefore, as we can clearly see, this is a regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import boston_housing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we'll import our data. We can see that this dataset is quite small, containing only 404 training items and 102 validation items. Input is a 13 dimensional vector, output is a number in thousands of dolars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: x-(404, 13) y-(404,) Test shape: x-(102, 13)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "print('Train shape: x-{} y-{} Test shape: x-{}'.format(x_train.shape, y_train.shape, x_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before puttingh our data into the network, it is always a good practice to normalize the data. Here, we use feature-wise normalization. For each input feature we substract the mean of the feature and divide by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = x_train.mean(axis=0)\n",
    "x_train -= mean\n",
    "std = x_train.std(axis=0)\n",
    "x_train /= std\n",
    "\n",
    "x_test -= mean\n",
    "x_test /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now proceed with building our model. We'll use 3 layers of fully-connected dense layer with 64 nodes and relu activation function for the first two layers. Last layer has only one node (since we are predicting only 1 number) and no activation function, as we don't want to affect or limit our final prediction. Since this is a regression task, we are using mean squared error loss function and mean absolute error as our metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=(x_train.shape[1],)))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a small amount of data, we'll use K-fold validation to reduce dependency on the data pick. We'll run 500 epochs so we are able to tell when does our network begin to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold #0\n",
      "Processing fold #1\n",
      "Processing fold #2\n",
      "Processing fold #3\n"
     ]
    }
   ],
   "source": [
    "k = 4\n",
    "num_val_samples = len(x_train) // k\n",
    "num_epochs = 500\n",
    "all_mae_histories = []\n",
    "\n",
    "for i in range(k):\n",
    "    print('Processing fold #{}'.format(i))\n",
    "    \n",
    "    val_data = x_train[i*num_val_samples : (i+1)*num_val_samples]\n",
    "    val_targets = y_train[i*num_val_samples : (i+1)*num_val_samples]\n",
    "    \n",
    "    partial_x_train = np.concatenate([x_train[: i*num_val_samples], x_train[(i+1)*num_val_samples:]],\n",
    "                                     axis=0)\n",
    "    partial_y_train = np.concatenate([y_train[: i*num_val_samples], y_train[(i+1)*num_val_samples:]],\n",
    "                                     axis=0)\n",
    "    model = build_model()\n",
    "    history = model.fit(partial_x_train, partial_y_train, \n",
    "              validation_data=(val_data, val_targets),\n",
    "              epochs=num_epochs, batch_size=1, verbose=0)\n",
    "    \n",
    "    mae_history = history.history['val_mean_absolute_error']\n",
    "    all_mae_histories.append(mae_history)\n",
    "\n",
    "average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our network is improving from the beginning, until about 80 epochs. Then, it begins to overfit on the training data, reducing its ability for correct predictions on new unseen validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def smooth_curve(points, factor=0.9):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points\n",
    "\n",
    "smooth_mae_history = smooth_curve(average_mae_history[10:])\n",
    "\n",
    "plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, based on our knowledge, we'll train our final model. We use 75 epochs with batch size increased to 16. As we can see, our MAE is about 2.657 which means that our predictions are off by an average of \\$2657."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 0s 575us/step\n",
      "MSE: 20.577824012905943, MAE: 2.8793123937120626\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.fit(x_train, y_train, epochs=75, batch_size=16, verbose=0)\n",
    "\n",
    "test_mse_score, test_mae_score = model.evaluate(x_test, y_test)\n",
    "print('MSE: {}, MAE: {}'.format(test_mse_score, test_mae_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
