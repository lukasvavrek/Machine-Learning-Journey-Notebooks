{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we are using simple NLP techniques to preprocess our data into the bag-of-words like dataset. Then, we are able to analyze sentiment based on the words used in the given review. This approach is not very robust, especially compared to RNN, but it's definitely better than guessing.\n",
    "\n",
    "http://www.cs.jhu.edu/~mdredze/datasets/sentiment/index2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T18:05:25.834562Z",
     "start_time": "2019-02-08T18:05:24.672441Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T18:05:26.496491Z",
     "start_time": "2019-02-08T18:05:25.837497Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_reviews = BeautifulSoup(open('../data/sorted_data_acl/electronics/negative.review').read())\n",
    "negative_reviews = [tag.text for tag in negative_reviews.findAll('review_text')]\n",
    "len(negative_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T18:05:27.149897Z",
     "start_time": "2019-02-08T18:05:26.499346Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_reviews = BeautifulSoup(open('../data/sorted_data_acl/electronics/positive.review').read())\n",
    "positive_reviews = [tag.text for tag in positive_reviews.findAll('review_text')]\n",
    "len(positive_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T18:05:27.160822Z",
     "start_time": "2019-02-08T18:05:27.154327Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = set(open('../data/general/english-stopwords.txt').read().split('\\n'))\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T18:05:29.365419Z",
     "start_time": "2019-02-08T18:05:27.165712Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(t):\n",
    "    words = re.split('; |, |\\*|\\n|/|_', t)\n",
    "    words = [word.strip(',.:-_').lower() for word in words]\n",
    "    words = [lemma.lemmatize(word) for word in words]\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "positive_reviews = [preprocess_text(r) for r in positive_reviews]\n",
    "negative_reviews = [preprocess_text(r) for r in negative_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T18:05:29.404253Z",
     "start_time": "2019-02-08T18:05:29.367251Z"
    }
   },
   "outputs": [],
   "source": [
    "positive_reviews = np.array(positive_reviews)\n",
    "negative_reviews = np.array(negative_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T18:05:29.444501Z",
     "start_time": "2019-02-08T18:05:29.406082Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((positive_reviews, negative_reviews))\n",
    "y = np.concatenate((np.ones(len(positive_reviews)), np.zeros(len(negative_reviews))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T18:05:29.750709Z",
     "start_time": "2019-02-08T18:05:29.446957Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T18:05:29.762821Z",
     "start_time": "2019-02-08T18:05:29.752706Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T18:05:29.777023Z",
     "start_time": "2019-02-08T18:05:29.766624Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-08T18:05:29.791878Z",
     "start_time": "2019-02-08T18:05:29.779932Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7954545454545454"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:development36]",
   "language": "python",
   "name": "conda-env-development36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "157.39999389648438px",
    "left": "463px",
    "right": "332.6000061035156px",
    "top": "142.8000030517578px",
    "width": "356.3999938964844px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
